{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"sieve_populars.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyM892ToFAjz1sNXSwYqs6Jz"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"VkipRu9IIkhb"},"source":["import os\n","import random\n","main_path = 'gdrive/MyDrive/bot/data'\n","corpus_with_populars_path = os.path.join(main_path, 'sentence_similarity/corpus_with_populars.txt')\n","corpus_with_populars2_path = os.path.join(main_path, 'sentence_similarity/corpus_with_populars2.txt')\n","popular_words2_path = os.path.join(main_path, 'sentence_similarity/popular_words2.json')\n","index_dict2_path = os.path.join(main_path, 'sentence_similarity/index_dict2.json')\n","pl_txt = os.path.join(main_path, 'sentence_similarity/pl.txt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zw4kS19xJgcJ"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nrxHJ9EhI8R5"},"source":["all_sentences = set()\n","with open(pl_txt, \"r\", encoding=\"utf-8\") as corpus:\n","    for line in corpus:\n","      all_sentences.add(line)\n","    print(len(corpus.readlines()))\n","    print(len(all_sentences))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"20eLtvAyJciE"},"source":["with open(corpus_with_populars2_path, 'w', encoding=\"utf-8\") as f:\n","  for sentence in all_sentences:\n","    f.write(sentence)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hmLGn_sGKXhd"},"source":["with open(corpus_with_populars2_path, 'r', encoding=\"utf-8\") as f:\n","  for i in range(10):\n","    print(f.readline())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YNBrdso2PGe6"},"source":["from collections import defaultdict\n","import re\n","from typing import DefaultDict\n","import json\n","\n","\n","def tokenize(line):\n","    text = line.lower()\n","    pattern = r\"\\s*\\w*\\s*\"\n","    return list(filter(None, re.findall(pattern, text)))\n","\n","\n","def create_popular_words_dictionary(corpus_path, popularity_path):\n","    popularity_dict: DefaultDict[str, int] = defaultdict(int)\n","    with open(corpus_path, \"r\", encoding=\"utf8\") as corpus:\n","        for line in corpus:\n","            line = line.rstrip()\n","            for word in tokenize(line):\n","                word = word.strip()\n","                popularity_dict[word] += 1\n","\n","    sorted_dict = {k: v for k, v in sorted(popularity_dict.items(), key=lambda item: item[1])}\n","    with open(popularity_path, \"w\", encoding=\"utf-8\") as f:\n","        json.dump(sorted_dict, f, ensure_ascii=False)\n","\n","create_popular_words_dictionary(corpus_with_populars2_path, popular_words2_path)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nJzlF_CaKvNe"},"source":["def create_indexes_dict():\n","    with open(corpus_with_populars2_path, \"r\", encoding=\"utf-8\") as corp:\n","        with open(popular_words2_path, \"r\", encoding=\"utf-8\") as pops:\n","            pops_dict = json.load(pops)\n","            keys_set = set(pops_dict.keys())\n","            indexes = defaultdict(list)\n","            for index, line in enumerate(corp):\n","                line = line.strip()\n","                for word in tokenize(line):\n","                    word = word.strip()\n","                    if word in keys_set:\n","                        indexes[word].append(index)\n","\n","            for word in indexes:\n","                val = indexes[word]\n","                random.shuffle(val)\n","                indexes[word] = val[:1000]\n","    with open(index_dict2_path, \"w\", encoding=\"utf-8\") as save:\n","        json.dump(indexes, save, ensure_ascii=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qtExnULXQfZJ"},"source":["create_indexes_dict()"],"execution_count":null,"outputs":[]}]}